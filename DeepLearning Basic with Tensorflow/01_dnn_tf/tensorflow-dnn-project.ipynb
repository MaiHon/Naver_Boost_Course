{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colab 사용자를 위한 안내\n",
    "\n",
    "해당 노트북은 **로컬** 환경에서 최적화 되어 있습니다. 로컬 환경에서 진행하시는 분들은 바로 학습을 진행하시면 됩니다.\n",
    "\n",
    "Colab 을 사용하시는 분들은 처음에 아래 주석을 해제하시고 한번 만 실행시켜주세요!\n",
    "\n",
    "* 주석을 해제하는 방법: 해당 영역을 선택하고, `Ctrl + /` 를 누르면 해당 영역의 주석에 해제됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()\n",
    "\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive', force_remount=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colab 을 사용하시는 분들은 아래 주석을 해제하시고 `folder` 변수 명에 프로젝트 디렉토리를 저장한 위치를 작성해주세요! 예를 들어, `01_dnn_tf` 의 위치가 \"내 드라이브 > colab_notebook > tensorflow\" 폴더 안에 있는 경우, \"colab_notebook/tensorflow\" 를 작성하시면 됩니다.\n",
    "\n",
    "```python\n",
    "folder = \"colab_notebook/tensorflow\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from pathlib import Path\n",
    "\n",
    "# # folder 변수에 구글드라이브에 프로젝트를 저장한 디렉토리를 입력하세요!\n",
    "# folder = \"\"\n",
    "# project_dir = \"01_dnn_tf\"\n",
    "\n",
    "# base_path = Path(\"/content/gdrive/My Drive/\")\n",
    "# project_path = base_path / folder / project_dir\n",
    "# os.chdir(project_path)\n",
    "# for x in list(project_path.glob(\"*\")):\n",
    "#     if x.is_dir():\n",
    "#         dir_name = str(x.relative_to(project_path))\n",
    "#         os.rename(dir_name, dir_name.split(\" \", 1)[0])\n",
    "# print(f\"현재 디렉토리 위치: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow 는 `1.14.0` 버전을 기준으로 합니다. Colab 사용시, 첫번째 코드를 실행해보시고 만약에 버전이 다르다면 두 번째 주석을 해제하고 실행해주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 첫번째 코드블록\n",
    "import tensorflow as tf\n",
    "print('tensorflow version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 두번째 코드블록\n",
    "# !pip install tensorflow==1.14.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network: FashoinMNIST Classifier\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1OSa44ql8zf9kq2r_D_Q0U1WWTkMPcCgm\" width=\"600px\" height=\"500px\" />\n",
    "\n",
    "* 이미지 출처: 네이버\n",
    "<br>\n",
    "스마트 렌즈는 여러분이 찍은 이미지가 어떤 옷인지 판별하고 쇼핑과 연결지어 검색까지 해줍니다. 이러한 기술은 어떻게 만들어 지는 것일까요? 물론 복잡한 기술이 들어가겠지만 여기에는 딥러닝 기술이 포함되어 있습니다. 이번 프로젝트에서는 10 종류의 의류와 관련된 이미지를 학습시키고 판별하는 모델을 만들어 볼것입니다.     \n",
    "\n",
    "이번 실습의 목표는 다음과 같습니다.\n",
    "- [Fashion-Mnist](https://github.com/zalandoresearch/fashion-mnist) 데이터셋을 활용해 분류기를 학습한다.\n",
    "- Multi layer perceptron, Batch normalization, ReLU 를 활용해 네트워크를 설계한다.\n",
    "\n",
    "이번 과정을 통해 여러분은 TensorFlow 를 이용해 분류기를 학습시키고, 학습된 모델의 성능을 검사하는 절차를 익힐 수 있습니다. 전체적인 과정은 다음과 같습니다.\n",
    "\n",
    "- 우리가 다뤄야 할 데이터는 28x28x1 (이미지 높이x이미지 너비x채널)의 흑백 이미지입니다. 즉, 밝기값만을 가지고 있습니다.\n",
    "    - 생활속에서 주로 접하게되는 컬러 이미지의 경우 빛의 3원색인 Red, Green, Blue의 3채널로 구성되어 있습니다. 일반적으로 이 3채널을 RGB채널이라고 부르며, 이 채널 값들의 조합으로 색상을 표현하게 됩니다.\n",
    "- DNN(Deep Neural Network)의 입력으로 사용되기 위해서 28$\\times$28$\\times$1의 3차원은 784의 1차원 데이터로(28\\*28\\*1=784) 변환됩니다. \n",
    "- 784차원의 입력 데이터는 DNN을 통과하여 10차원의 의류 종류를 나타내는 출력으로 변환 됩니다(아래의 그림을 참고해 주세요).\n",
    "- 여러분이 만들어야 하는 것은 이 DNN 구조를 TensorFlow를 이용하여 설계하는 과정입니다.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1N691obHfLeKvP7eJ842EkMovK0O_Nu5E\" width=\"800px\" height=\"300px\"/>\n",
    "<caption><center>&lt;28$\\times$28$\\times$1의 이미지를 입력으로 받아 옷의 종류를 반환하는 DNN&gt;</center></caption>\n",
    "\n",
    "### 이제부터 본격적으로 프로젝트를 시작하겠습니다.\n",
    "\n",
    "**\"[TODO] 코드 구현\"** 부분의 **\"## 코드 시작 ##\"** 부터 **\"## 코드 종료 ##\"** 구간에 필요한 코드를 작성해주세요. **나머지 작성구간이 명시 되지 않은 구간은 임의로 수정하지 마세요!**\n",
    "\n",
    "실습코드는 Python 3.6, TensorFlow 1.14.0 버전을 기준으로 작성되었습니다.\n",
    "\n",
    "**본문 중간중간에 TensorFlow 함수들에 대해 [TensorFlow API 문서](https://www.tensorflow.org/api_docs/python/tf) 링크를 걸어두었습니다. API 문서를 직접 확인하는 일에 익숙해지면 나중에 여러분이 처음부터 모델을 직접 구현해야 할 때 정말 큰 도움이 됩니다.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>목차<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Colab-사용자를-위한-안내\" data-toc-modified-id=\"Colab-사용자를-위한-안내-1\">Colab 사용자를 위한 안내</a></span></li><li><span><a href=\"#Neural-Network:-FashoinMNIST-Classifier\" data-toc-modified-id=\"Neural-Network:-FashoinMNIST-Classifier-2\">Neural Network: FashoinMNIST Classifier</a></span><ul class=\"toc-item\"><li><span><a href=\"#1.-Package-load\" data-toc-modified-id=\"1.-Package-load-2.1\">1. Package load</a></span></li><li><span><a href=\"#2.-하이퍼파라미터-세팅\" data-toc-modified-id=\"2.-하이퍼파라미터-세팅-2.2\">2. 하이퍼파라미터 세팅</a></span></li><li><span><a href=\"#3.-Dataset-load-및-tf.data.Dataset-구축\" data-toc-modified-id=\"3.-Dataset-load-및-tf.data.Dataset-구축-2.3\">3. Dataset load 및 <code>tf.data.Dataset</code> 구축</a></span></li><li><span><a href=\"#4.-데이터-샘플-시각화\" data-toc-modified-id=\"4.-데이터-샘플-시각화-2.4\">4. 데이터 샘플 시각화</a></span></li><li><span><a href=\"#5.-모델-(네트워크)-만들기\" data-toc-modified-id=\"5.-모델-(네트워크)-만들기-2.5\">5. 모델 (네트워크) 만들기</a></span></li><li><span><a href=\"#6.-Loss-function-및-Optimizer-정의\" data-toc-modified-id=\"6.-Loss-function-및-Optimizer-정의-2.6\">6. Loss function 및 Optimizer 정의</a></span></li><li><span><a href=\"#7.-Training\" data-toc-modified-id=\"7.-Training-2.7\">7. Training</a></span></li><li><span><a href=\"#8.-Evaluate-on-test-dataset\" data-toc-modified-id=\"8.-Evaluate-on-test-dataset-2.8\">8. Evaluate on test dataset</a></span></li><li><span><a href=\"#9.-Summary\" data-toc-modified-id=\"9.-Summary-2.9\">9. Summary</a></span></li></ul></li><li><span><a href=\"#Self-Review\" data-toc-modified-id=\"Self-Review-3\">Self-Review</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Package load\n",
    "\n",
    "먼저, 필요한 패키지들을 로드합니다.\n",
    "주로 사용될 대표적인 패키지들의 사용목적은 다음과 같습니다.\n",
    "\n",
    "- [`numpy`](https://www.numpy.org/): Scientific computing과 관련된 여러 편리한 기능들을 제공해주는 라이브러리입니다.\n",
    "- [`matplotlib.pyplot`](https://matplotlib.org/): 데이터 시각화를 위해 사용합니다.\n",
    "- [`tensorflow`](https://www.tensorflow.org/): TensorFlow 를 로드합니다.\n",
    "- [`tensorflow.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers): 모델의 각 Layer들을 만들기 위해 사용합니다.\n",
    "- [`tf.enable_eager_execution()`](https://www.tensorflow.org/api_docs/python/tf/enable_eager_execution): 향후 TF version 2.0에 기본이 될 eager execution 모드를 위해 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import check_util.checker as checker\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow as tf\n",
    "from IPython.display import clear_output\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "print('tensorflow version: {}'.format(tf.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(tf.test.is_gpu_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 하이퍼파라미터 세팅\n",
    "\n",
    "학습에 필요한 하이퍼파라미터의 값을 초기화해줍니다. 하이퍼파라미터는 뉴럴네트워크를 통하여 학습되는 것이 아니라 학습율(learning rate), 사용할 레이어의 수 등 설계자가 결정해줘야 하는 값들을 의미합니다.\n",
    "\n",
    "미니배치의 크기(`batch_size`), 학습 할 epoch 수(`max_epochs`), 학습률(`learning_rate`) 등의 값들을 다음과 같이 정했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "max_epochs = 5\n",
    "learning_rate = 0.001\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset load 및 `tf.data.Dataset` 구축\n",
    "\n",
    "[Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist)는 10개의 필기체 숫자로 구성된 [MNIST 데이터](http://yann.lecun.com/exdb/mnist/)의 Fashion 버젼(version)으로 보시면 됩니다. 프로그래밍을 처음 접해보는 사람들이 주로 가장 먼저 실습해 보는 것이 \"Hello, World\"를 출력해보는 것이죠. 머신러닝을 처음 접해보시는 분들에게 이 \"Hello, Wolrd\"를 출력해보는 작업이 MNIST 데이터를 분류해보는 것이라고 비유할 수 있을 것 같습니다. 그만큼 많은 사람들이 쉽게 다운로드 받아 테스트 해볼 수 있고, 분류기들의 성능 비교용으로도 많이 사용됩니다.<br>\n",
    "그러나 현재 MNIST 분류는 머신러닝 기술의 발전하면서 너무 쉬운 문제가 됐고, MNIST에서 좋은 성능을 보이는 분류기가 다른 데이터에서도 잘 작동한다고 말하기 어려운 환경이 됐습니다. 그에대한 한가지 대안으로 제시된 데이터가 Fashion-MNIST 입니다. <br>\n",
    "Fashion-MNIST는 MNIST와 동일한 크기의 데이터(10개의 부류, 60,000개의 학습, 10,000개의 테스트 데이터)이지만 MNIST보다 분류하기 어려운 의류 영상데이터 입니다. Fashion-MNIST의 데이터 부류는 T-Shirts, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Bag, Ankle boot로 10개의 입니다. \n",
    "\n",
    "실습을 위해 Fashion-MNIST 데이터셋을 정의해주고, 전체 데이터셋을 미니배치 단위로 뉴럴넷에 공급해주도록 `tf.data.Dataset`을 정의합니다.\n",
    "* `tf.data.Dataset`에 대한 자세한 설명은 [Importing Data](https://www.tensorflow.org/guide/datasets) 페이지를 참조하세요.\n",
    "\n",
    "### Fashion-MNIST 데이터셋 load\n",
    "\n",
    "* [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) 데이터는 `tf.keras.datasets`에서 기본 제공됩니다.\n",
    "* 아래 코드블록의 5번째 줄은 `train_data`의 값의 범위인 [0, 255]의 범위를 [0, 1]의 범위로 조절 합니다.\n",
    "* 6번째 줄은 이미지 형태의 (28(높이), 28(너비)) 데이터를 네트워크의 입력으로 넣기 위해 1차원의 28*28=784 데이터로 변경합니다.\n",
    "* 10번째 줄부터 `train_data`를 변환한 방식으로 `test_data`를 변환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and eval data from tf.keras\n",
    "(train_data, train_labels), (test_data, test_labels) = \\\n",
    "    tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "train_data = train_data / 255.\n",
    "train_data = train_data.reshape([-1, 28 * 28])\n",
    "train_data = train_data.astype(np.float32)\n",
    "train_labels = train_labels.astype(np.int32)\n",
    "\n",
    "test_data = test_data / 255.\n",
    "test_data = test_data.reshape([-1, 28 * 28])\n",
    "test_data = test_data.astype(np.float32)\n",
    "test_labels = test_labels.astype(np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "`tf.data.Dataset`을 이용하여 input pipeline 구축합니다.\n",
    "\n",
    "TensorFlow의 tf.data API는 네트워크 입력을 만들기 위한 복잡한 데이터 처리과정을 단순화 시켜줍니다.\n",
    "본 실습에서는 데이터 입력 파이프라인을 쉽게 설계할 수 있는 tf.data.Dataset을 이용할 겁니다.\n",
    "실습에 사용될 tf.data.Dataset 내에서 사용할 API들은 다음과 같습니다.\n",
    "\n",
    "* `tf.data.Dataset.from_tensor_slices`: numpy타입의 데이터를 tf.data.Dataset 형태의 데이터로 변환시켜줍니다.\n",
    "* `tf.data.Dataset.shuffle`: 데이터를 무작위로 섞어 줍니다.\n",
    "  * `shuffle`함수의 매개변수로 `buffer_size`가 있습니다. 이는 버퍼를 `buffer_size` 크기의 요소(elements)로 채운 다음 이 버퍼에서 무작위로 샘플링하고 선택된 요소를 새로운 요소로 대체합니다. 완벽한 섞임을 위해서는 데이터 세트의 전체 크기보다 크거나 같은 버퍼 크기(`buffer_size`)가 필요합니다.\n",
    "* `tf.data.Dataset.batch`: 읽어들일 데이터의 배치크기(batch_size)를 결정합니다.\n",
    "\n",
    "**tf.data.Dataset API를 이용하여 다음과 같은 작업을 수행해야 합니다**\n",
    "* [`tf.data.Dataset.from_tensor_slices`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#from_tensor_slices) API를 이용하여 train_data 및 train_labels데이터를 `tf.data.Dataset` 형태로 만들어봅시다. 마찬가지로 test_data, test_labels데이터도 `tf.data.Dataset` 형태로 만들어봅니다. 이미지 한장에 해당하는 부류 레이블이 한 세트로 묶어져야 합니다.\n",
    "* [`shuffle`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#shuffle), [`batch`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#batch)등을 이용하여 효율적인 데이터입력 파이프라인을 만들어봅시다. **<2. 하이퍼파라미티 세팅>** 에서 정의한 `batch_size` 변수를 활용하세요!\n",
    "* `tf.data.Dataset`의 다양한 메소드(method)들은 [`tf.data.Dataset`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) 페이지에서 살펴보시면 좋습니다.\n",
    "\n",
    "**아래의 코드블록은 tf.data.Dataset을 만드는 간단한 에제와 데이터 추출과정을 보여줍니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data.Dataset 만드는 간단한 예제\n",
    "temp_dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3, 4])\n",
    "temp_dataset = temp_dataset.shuffle(60000)\n",
    "temp_dataset = temp_dataset.batch(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실제 data를 추출하는 과정\n",
    "for epoch in range(3):\n",
    "    for step, data in enumerate(temp_dataset):\n",
    "        print(\"epoch: {}  step: {}  data: {}\".format(\n",
    "              epoch+1, step+1, data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test`데이터 셋은 shuffle할 필요가 없습니다. \n",
    "* `train`시 shuffle하는 목적은 mini-batch gradient descent를 하기 위해 mini-batch 데이터를 random 하게 뽑는 것입니다. \n",
    "* `test` 데이터 셋의 목적은 성능을 평가하기 위함입니다. 그렇기 때문에 `test`데이터 셋은 shuffle할 필요가 없습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이제 모델에게 전달할 데이터 공급 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for train\n",
    "N = len(train_data)\n",
    "## 코드 시작 ##\n",
    "train_dataset = None\n",
    "train_dataset = None\n",
    "train_dataset = None\n",
    "## 코드 종료 ##\n",
    "print(train_dataset)\n",
    "\n",
    "# for test\n",
    "## 코드 시작 ##\n",
    "test_dataset = None\n",
    "test_dataset = None\n",
    "## 코드 종료 ##\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`print(train_dataset)` 및 `print(test_dataset)`의 결과가 다음과 같다면 잘 만들었을 가능성이 높습니다.\n",
    "\n",
    "```\n",
    "<DatasetV1Adapter shapes: ((?, 784), (?,)), types: (tf.float32, tf.int32)>\n",
    "<DatasetV1Adapter shapes: ((?, 784), (?,)), types: (tf.float32, tf.int32)>\n",
    "```\n",
    "\n",
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.train_dataset_check(train_dataset)\n",
    "checker.test_dataset_check(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 데이터 샘플 시각화\n",
    "\n",
    "아래의 코드블록은 Fashion-MNIST 데이터를 시각화 합니다.\n",
    "\n",
    "`labels_map`에 각 클래스의 이름과 인덱스를 파이썬 딕셔너리(dictionary)로 저장했습니다. \n",
    "7번재 줄부터 for문을 이용하여 `train_data`에 속한 데이터를 무작위로 25개 추출한 뒤 5$\\times$5 격자 형태로 출력합니다.\n",
    "\n",
    "`matplotlib`은 2D 시각화를 위한 라이브러리입니다. `matplotlib.pyplot`의 figure()를 통해 그림을 그릴 도화지를 생성할 수 있습니다. 생성된 figure 객체의 `add_subplot` 함수를 통해 전체 도화지 속에 일부 도면을 삽입할 수 있습니다. 큰 도화지 위에 작은 그림들을 구역마다 그리는 것입니다. 코드를 실행해 생성된 그림을 먼저 보시고 코드를 함께 보면 더욱 이해하기 쉬울 것입니다.\n",
    "`matplotlib.pyplot`의 더 다양한 기능을 살펴보고 싶으면 [이곳](https://matplotlib.org/3.1.0/tutorials/introductory/pyplot.html)을 참고해주세요. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_map = {0: 'T-Shirt', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat',\n",
    "              5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle Boot'}\n",
    "columns = 5\n",
    "rows = 5\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "\n",
    "for i in range(1, columns*rows+1):\n",
    "    data_idx = np.random.randint(len(train_data))\n",
    "    img = train_data[data_idx].reshape([28, 28])\n",
    "    label = labels_map[train_labels[data_idx]]\n",
    "\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.title(label)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 모델 (네트워크) 만들기\n",
    "\n",
    "학습시킬 뉴럴네트워크를 설계합니다. \n",
    "이번 실습에서는 Multi Layer Perceptron(MLP) 레이어를 2개 쌓아 네트워크를 설계할 것입니다.\n",
    "\n",
    "MLP는 아래의 그림과 같이 한 레이어의 모든 뉴런이 다음 레이어의 뉴런과 완전히 연결된 계층(Fully connected layer 또는 Dense layer)입니다. \n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1KuQg548RFXMm1Kih46IXkKLO-q76lBdQ\" width=\"800px\" height=\"400px\" />\n",
    "\n",
    "\n",
    "한편, MLP의 레이어를 깊게 쌓을 때에는 반드시 비선형 활성화 함수(activation function)이 필요합니다.\n",
    "이번 실습에서는 ReLU(Rectified Linear Unit)를 사용 할 것입니다. \n",
    "ReLU는 아래의 그림과 같이 음수의 입력에 대해서는 0, 양수의 입력에 대해서는 입력값을 그대로 출력하는 함수입니다.\n",
    "\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=14hYX4UF0Ony8apMZmN7IEkQHqP6PB-ne\" width=\"400px\" height=\"400px\" /><caption><center>ReLU의 출력</center></caption>\n",
    "\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요.\n",
    "\n",
    "- 첫번째 dense layer의 입력 feature 갯수는 입력 이미지의 픽셀 갯수인 28 $\\times$ 28로, 출력 feature 갯수는 512로 하겠습니다.\n",
    "- 첫번째 dense layer와 ReLU 사이에 Batch normalization(['Lab-10-4'](https://www.youtube.com/watch?v=-VwtLBp2FRs&list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C&index=30))을 적용해보세요.\n",
    "- 그리고 첫번째 dense layer 이후에는 ReLU 함수를 적용해보세요.\n",
    "- 두번째 dense layer의 출력 feature 갯수는 데이터의 class 갯수인 10으로 지정해야 합니다. (혹은 `2. 하이퍼파라미터 세팅`장에서 정의한 `num_classes` 로 지정합니다.)\n",
    "- 두번째 dense layer 이후에는 활성함수를 적용하지 않습니다. 분류를위한 네트워크의 마지막 활성함수는 주로 softmax 함수가 적용되기 때문입니다. 그렇다면 왜 softmax 함수는 TensorFlow를 통한 네트워크 구현에 포함시키지 않는걸까요? 그 이유는 우리가 이후에 선언할 크로스엔트로피(Cross Entropy) [`tf.keras.losses.CategoricalCrossentropy()`](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy) 함수에서 **logits**을 받아 계산하기 위해서 입니다. (여기서 **logits**은 강의 동영상에서의 **score**랑 같은 개념이라 생각하시면 됩니다) 실제 `CategoricalCrossentropy()` 손실함수를 사용 할때 `from_logits=True` 옵션을 주셔야 합니다. (ex. `tf.keras.losses.CategoricalCrossentropy(from_logits=True)`).  따라서 softmax 함수는 우리가 따로 선언하지 않아도 됩니다.\n",
    "\n",
    "- 실습에 사용 될 **tf.keras.layers API** (**자세한 사용법은 레이어명 클릭**)\n",
    "  - [`Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) : 일반적인 완전연결(densely-connected) 레이어\n",
    "  - [`BatchNormalization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization) : 배치 노말라이제이션 레이어\n",
    "  - [`ReLU`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU) : ReLU 활성화 함수 레이어"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`tf.keras.Sequential()`](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)을 이용하여 모델 만들기\n",
    "\n",
    "* `tf.keras.model.Sequential()`와 `tf.keras.Sequential()`은 같은 API 입니다.\n",
    "* `Sequential`은 해당 레이어의 출력이 그대로 다음 레이어의 입력이 되는 구조의 모델을 만들 때 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential() # Sequential 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**tf.keras.layers API 를 이용해 모델 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 코드 시작 ##\n",
    "model.add(None) # 첫번째 dense 레이어\n",
    "model.add(None) # Batch normalization 레이어\n",
    "model.add(None) # ReLU 레이어\n",
    "model.add(None) # 두번째 dense 레이어\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [`Functional API`](https://www.tensorflow.org/guide/keras#build_advanced_models)를 이용하여 모델 만들기\n",
    "\n",
    "* `tf.keras.Sequential()`은 다음과 같은 모델을 만들지 못합니다.\n",
    "  * Multi-input models\n",
    "  * Multi-output models\n",
    "  * 레이어 공유 모델 (같은 레이어를 여러번 반복하는 모델)\n",
    "  * Sequential 하지 않은 모델 (e.g. residual connections) \n",
    "* `Functional API`는 `tf.keras.Sequential()`보다 훨씬 자유롭게 더욱 일반적인 모델을 만들 수 있습니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "위의 `tf.keras.Sequential()`로 만든 모델과 같은 구조의 모델을 `Functional API`를 사용하여 만들어보세요.\n",
    "* Model의 `call` 함수 부분을 작성할 때 `training` 매개변수를 잘 고려해서 작성해보세요. 강의영상을 참조하고 싶으시면 [여기](https://www.youtube.com/watch?v=pAFnPKFHvqE&list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C&index=27)를 클릭하세요. 모델 객체를 선언할 때 만들었던 `Batch normalization` 과 `Dropout` 층을 통과 시 `training` 매개변수를 같이 전달해야 합니다.\n",
    "\n",
    "> 모델에 `training` 매개변수를 `False`로 주는 이유는 다음과 같습니다. `Batch normalization`, `Dropout`과 같은 레이어들은 학습을 할 때와 테스트 할 때 작동하는 방식이 다르기 때문입니다. 그래서 현재 `mode`가 `training` 중인지 아닌지를 매개변수로 넘겨주는 것입니다.\n",
    "\n",
    "* 사용 예는 다음과 같습니다. \n",
    "\n",
    "```python\n",
    "class BN_Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BN_Model, self).__init__()\n",
    "        self.batch_norm = tf.keras.layers.BatchNormalization()\n",
    "    \n",
    "    def call(self, x, training=False):\n",
    "        return self.batch_norm(x, training)\n",
    "\n",
    "model = BN_Model()\n",
    "\n",
    "# Training 할 때\n",
    "model(x, training=True)\n",
    "\n",
    "# Test (or Validate) 할 때\n",
    "model(x, training=False) \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functional API 를 이용해 모델 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNNModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(DNNModel, self).__init__()\n",
    "        ## 코드 시작 ##\n",
    "        self.dense1 = None\n",
    "        self.bn1 = None\n",
    "        self.relu1 = None\n",
    "        self.dense2 = None\n",
    "        ## 코드 종료 ##\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        \"\"\"Run the model.\"\"\"\n",
    "        ## 코드 시작 ##\n",
    "        dense1_out = None\n",
    "        bn1_out = None\n",
    "        relu1_out = None\n",
    "        dense2_out = None\n",
    "        ## 코드 종료 ##\n",
    "        return dense2_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DNNModel(num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터의 일부를 넣어서 model 체크 & summary 하기\n",
    "\n",
    "모델을 학습 하기전에 모델이 잘 동작하는지 확인해 보겠습니다. 아래의 코드블록은 배치크기만큼 영상을 가져온 후 이 중 3개를 영상을 모델에 입력해 보는 코드입니다. 그리고 `model.summary()`를 통해 생성된 모댈의 정보를 확인하실 수 있을 겁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without training, just inference a model in eager execution:\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(\"predictions: \", model(images[0:3]))\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.model_check(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Loss function 및 Optimizer 정의\n",
    "\n",
    "생성한 모델을 학습 시키기 위해서 손실함수를 정의해야 합니다. 뉴럴네트워크는 경사하강(gradient descent)방법을 이용하여 손실함수의 값을 줄이는 방향으로 파라미터를 갱신(update) 하게 됩니다. 또한 효과적인 경사하강 방법을 적용하기 위해 옵티마이져를 함께 사용할 겁니다.\n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요.API의 사용법은 링크를 클릭하셔서 확인하실 수 있습니다.\n",
    "\n",
    "- `loss_object` 변수에 classification에서 자주 사용되는 [Cross Entropy Loss](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)를 정의하세요.\n",
    "- `optimizer` 변수에 [Adam optimizer](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)를 정의하세요. **<2. 하이퍼파라미터 세팅>** 에서 정의한 `learning_rate` 를 사용하세요.\n",
    "\n",
    "**이제 손실함수와 옵티마이저 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 코드 시작 ##\n",
    "loss_object = None\n",
    "optimizer = None\n",
    "## 코드 종료 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델이 얼마나 정확하게 정답을 찾아내는지 확인하게 위해서 [tf.keras.metrics.Accuracy()](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Accuracy)을 정의하고, 손실값의 평균값들을 계산하기 위해서 [tf.keras.metrics.Mean()](https://www.tensorflow.org/api_docs/python/tf/keras/metrics/Mean)을 정의했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy = tf.keras.metrics.Accuracy(\"accuracy\")\n",
    "mean_loss = tf.keras.metrics.Mean(\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `tf.keras.metrics.Accuracy`와 `tf.keras.metrics.Mean`의 간단한 사용법\n",
    "\n",
    "`tf.keras.metrics.Accuracy()`와 `tf.keras.metrics.Mean()`모두 **누적** 정확도 및 평균 값을 계산해주는 함수입니다.\n",
    "\n",
    "먼저 `tf.keras.metrics.Accuracy()` 사용법을 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_accuracy_ex = tf.keras.metrics.Accuracy(\"example_accuracy\")\n",
    "print(mean_accuracy_ex([0, 1, 2, 4], [0, 1, 3, 4])) # 0.75 = 3/4\n",
    "print(mean_accuracy_ex([5, 1, 7, 3], [5, 0, 2, 7])) # 0.5 = 4/8 위의 계산에서 부터 누적"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그 다음 `tf.keras.metrics.Mean()` 사용법을 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ex = tf.keras.metrics.Mean(\"example_mean\")\n",
    "# mean(values) 형태로 사용합니다.\n",
    "print(mean_ex([0, 1, 2, 3])) # 1.5 = (0 + 1 + 2 + 3) / 4\n",
    "print(mean_ex([4, 5])) # 2.5 = (0 + 1 + 2 + 3 + 4 + 5) / 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래의 코드를 실행해 코드를 성공적으로 완성했는지 확인해보세요. \n",
    "\n",
    "별다른 문제가 없다면 이어서 진행하면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checker.loss_function_check(loss_object)\n",
    "checker.optimizer_check(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training\n",
    "\n",
    "이제 모델에 데이터를 미니배치 단위로 제공해서 학습을 시킬 단계입니다. \n",
    "\n",
    "### <font color='red'>[TODO] 코드 구현</font>\n",
    "\n",
    "다음을 읽고 코드를 완성해보세요. **<6. Loss function 및 Optimizer 정의>** 에서 만든 객체인  `model`, `loss_object`, `optimizer`를 활용하셔야 합니다. \n",
    "\n",
    "`tf.data.Dataset`으로 만든 객체인 `train_dataset`을 `for`문에 넣으면 배치 단위의 데이터를 받을 수 있습니다. `for`문을 이용하여 `train_dataset`으로부터 데이터를 획득하고 학습하는 과정을 아래 설명대로 완성하세요.\n",
    "\n",
    "1. 모델에 `images` (`입력 데이터`)를 주고 그 출력을 `predictions` 변수에 저장하세요.\n",
    "2.  모델의 `predictions`과 `train_dataset`에서 읽어드릴 `labels`를 통해 loss를 구하고, 그 결과를 `loss_value` 변수에 저장하세요.\n",
    "    - 여기서 중요한 것은 `predictions`의 shape은 (batch_size, 10)이고 `labels`의 shape은 (batch_size,)라는 점입니다. 서로 shape이 다르기 때문에 실제 loss 계산을 위해서 `labels`을 one-hot 벡터로 바꾸어 주어 `loss_object`에 넣어줘야 합니다.\n",
    "    - one-hot 벡터의 자세한 설명은 [Lab06-2](https://www.youtube.com/watch?v=rzxT1a0IYz8&list=PLQ28Nx3M4Jrguyuwg4xe9d9t2XE639e5C&index=15)에 잘 나와 있습니다.\n",
    "3. `tape`로 정의된 [`tf.GradientTape()`](https://www.tensorflow.org/api_docs/python/tf/GradientTape) 객체의 `gradient` 메서드를 이용하여 `loss_value`를 `model.trainable_variables`로 미분하고, 이 값을 `gradients` 변수에 저장합니다.\n",
    "4. [`optimizer.apply_gradients()`](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer#methods)를 이용하여 파라미터를 업데이트 합니다. 함수 인자로 `gradients`와 `model.trainable_variables`를 `zip`으로 묶어서 전달하면 됩니다.\n",
    "\n",
    "\n",
    "**이제 각 스텝에 따라 훈련 단계 코드를 작성해보세요! \"<font color='45A07A'>## 코드 시작 ##</font>\"과 \"<font color='45A07A'>## 코드 종료 ##</font>\" 사이의 <font color='075D37'>None</font> 부분을 채우시면 됩니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_per_epoch = len(list(train_dataset))\n",
    "for epoch in range(max_epochs):\n",
    "    for step, (images, labels) in enumerate(train_dataset):\n",
    "        start_time = time.time()\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            ## 코드 시작 ##\n",
    "            predictions = None    # 위의 설명 1. 을 참고하여 None을 채우세요.\n",
    "            loss_value = None     # 위의 설명 2. 를 참고하여 None을 채우세요.\n",
    "\n",
    "        gradients = None                   # 위의 설명 3. 을 참고하여 None을 채우세요.\n",
    "        optimizer.apply_gradients(None)    # 위의 설명 4. 를 참고하여 None을 채우세요.\n",
    "        ## 코드 종료 ##\n",
    "        \n",
    "        mean_loss(loss_value)\n",
    "        mean_accuracy(labels, tf.argmax(predictions, axis=1))\n",
    "        \n",
    "        if (step+1) % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            epochs = epoch + step / float(num_batches_per_epoch)\n",
    "            duration = time.time() - start_time\n",
    "            examples_per_sec = batch_size / float(duration)\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%, ({:.2f} examples/sec; {:.3f} sec/batch)'.format(\n",
    "                epoch+1, max_epochs, step+1, num_batches_per_epoch, mean_loss.result(), \n",
    "                mean_accuracy.result() * 100, examples_per_sec, duration))\n",
    "\n",
    "    # clear the history\n",
    "    mean_loss.reset_states()\n",
    "    mean_accuracy.reset_states()\n",
    "\n",
    "print(\"training done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 로그에 출력되는 마지막 학습 Accuracy가 90% 전후로 나오면 코드를 잘 완성한 것입니다. 성능이 안나온다면 **<2. 하이퍼파라미터 세팅>** 에서 지정한 `max_epochs`, `learning_rate` 의 하이퍼파라미터 숫자를 조절해보세요! \n",
    "\n",
    "만약에 학습이 진행이 되지 않는다면 지문과 지문에 나와있는 API문서 링크를 다시 한 번 꼼꼼히 살펴보시기 바랍니다.\n",
    "\n",
    "문제가 없다면 다음으로 이어서 진행하시면 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluate on test dataset\n",
    "\n",
    "마지막으로 학습된 모델의 성능을 테스트할 차례입니다.\n",
    "\n",
    "> `model(images, training=False)`\n",
    "\n",
    "모델에 `training` 인자를 `False`로 주는 이유는 다음과 같습니다. `Batch normalization`, `Dropout`과 같은 레이어들은 훈련 단계와 테스트 단계에서 작동하는 방식이 다르기 때문입니다. 그래서 현재 `mode`가 `training`중인지 아닌지를 불리언(Boolean, 참 혹은 거짓) 인자로로 넘겨주는 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in test_dataset:\n",
    "    predictions = model(images, training=False)\n",
    "    mean_accuracy(labels, tf.argmax(predictions, axis=1))\n",
    "\n",
    "print(\"test accuracy: {:.2f}%\".format(mean_accuracy.result() * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종 성능이 대략 88% 전후로 나오면 학습이 잘된 것으로 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습된 모델의 예측 결과를 시각화하면 다음과 같습니다. label이 <font color='blue'>파란색</font>으로 표시되면 모델이 정확한 예측을 한 것이고 <font color='red'>빨간색</font>으로 표시되면 틀린 예측을 한 것입니다. 틀린 경우에는 모델의 예측과 함께 실제 정답을 표기해두었습니다. (ex. 오답/정답)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_size = 25\n",
    "batch_index = np.random.choice(\n",
    "    len(test_data), size=test_batch_size, replace=False)\n",
    "\n",
    "batch_xs = test_data[batch_index]\n",
    "batch_ys = test_labels[batch_index]\n",
    "y_pred_ = model(batch_xs, training=False)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "for i, (px, py, y_pred) in enumerate(zip(batch_xs, batch_ys, y_pred_)):\n",
    "    p = fig.add_subplot(5, 5, i+1)\n",
    "    if np.argmax(y_pred) == py:\n",
    "        p.set_title(\"{}\".format(labels_map[py]), color='blue')\n",
    "    else:\n",
    "        p.set_title(\"{}/{}\".format(labels_map[np.argmax(y_pred)],\n",
    "                                   labels_map[py]), color='red')\n",
    "    p.imshow(px.reshape(28, 28), cmap=\"gray\")\n",
    "    p.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary\n",
    "\n",
    "여기까지 오신 여러분 잘하셨습니다! \n",
    "\n",
    "우리는 이번 실습을 통해 다음과 같은 내용을 학습했습니다.\n",
    "\n",
    "- Multi layer perceptron을 설계할 수 있다.\n",
    "- 네트워크에 ReLU, Batch normalization를 적용할 수 있다.\n",
    "- `tf.data.Dataset`을 이용하여 데이터입력 파이프라인(input pipeline)을 만들 수 있다.\n",
    "- 손실함수(loss function)과 옵티마이져(optimizer)를 정의할 수 있다.\n",
    "- 손실(loss)를 측정하고 경사(gradient)를 계산해 모델 파라미터를 업데이트할 수 있다.\n",
    "- 학습한 모델의 성능을 테스트 할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Review\n",
    "\n",
    "학습 환경에 맞춰 알맞는 제출방법을 실행하세요!\n",
    "\n",
    "### 로컬 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 혹은 `File > Save and checkpoint`로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 주의할 점은 Jupyter Notebook 의 파일이름을 수정하시면 안됩니다! 만약에 노트북 이름을 수정했다면 \"tensorflow-dnn-project\" 로 바꿔주시길 바랍니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"submit.zip\"이 생깁니다. \"dnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"dnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "    * \"dnn_submission.html\" : 여러분이 작성한 Jupyter Notebook 을 html 형식으로 전환한 파일\n",
    "3. 코드 실행결과 안내에 따라서 `submit.zip` 파일을 확인하시고 제출해주시길 바랍니다.\n",
    "\n",
    "### Colab 환경 실행자\n",
    "\n",
    "1. 모든 실습 완료 후, Jupyter Notebook 을 `Ctrl+S` 로 저장합니다.\n",
    "2. 제일 하단의 코드를 실행합니다. 코드 실행결과 안내에 따라서 재작성하거나 다음스텝으로 넘어갑니다. 모든 평가 기준을 통과하면, 함수 실행 후 프로젝트 \"submit\" 디렉토리와 압축된 \"dnn_submission.tsv\"만 생깁니다. \"dnn_submission.tsv\" 파일을 열고 모두 Pass 했는지 확인해보세요!\n",
    "    * \"dnn_submission.tsv\" : 평가 기준표에 근거해 각 세부항목의 통과여부(Pass/Fail) 파일\n",
    "3. 프로젝트를 저장한 드라이브의 `submit` 폴더에서 `dnn_submission.tsv` 파일을 다운 받습니다.\n",
    "4. Colab Notebook 에서 `파일 > .ipynb 다운로드`를 통해서 노트북을 다운로드 받습니다.\n",
    "5. 로컬에서 Jupyter Notebook 프로그램을 실행시킵니다. \n",
    "6. 4번 스텝에서 다운받은 노트북을 열고 `File > Download as > HTML(.html)` 로 재 다운로드 합니다.\n",
    "7. 3번 스텝에서 받은 파일과 6번 스텝에서 받은 파일을 하나의 폴더에 넣고, `submit.zip` 이라는 이름으로 압축하고 제출해주시길 바랍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_util.submit as submit\n",
    "submit.process_submit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "목차",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "349.091px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
